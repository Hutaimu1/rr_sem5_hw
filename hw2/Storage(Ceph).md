# Ceph
## 供应商
RedHat
## 特征
一个Ceph集群由两种类型的后台进程组成：
### Ceph OSD Daemon
Object Storage Device（OSD）是Ceph集群中的重要组成部分。OSD可以存储文件或数据的内容，它使用文件系统来存储数据。OSD Daemon主要负责管理集群中的所有磁盘。OSD Daemon还负责在本地文件系统存储数据，并为不同的客户软件或存取媒介通过网络提供数据访问。而且，OSD Daemon还负责添加和删除磁盘，磁盘分区，管理OSD、低层空间管理，提供安全措施和磁盘数据的可复制性
### Ceph Monitor
Ceph Monitor也是一种Ceph OSD Daemon，它主要负责管理全部集群。当你运行一个Ceph集群时，就会需要Ceph Monitor每天检查集群的健康情况和状态。管理一个集群需要每天做很多工作比如检测所有OSD的状态和文件系统或块数据的状态。可以通过Ceph Monitor来管理负载均衡和数据响应的详细信息。
## 处理数据存储的机制
### Ceph Object storage
当向Ceph写入数据时，Ceph通过内部机制自动跨集群标记和复制数据。Ceph存储对象数据时，不仅可以通过调用Ceph内部的API来实现，还可以通过亚马逊的S3服务或AWS REST提供的API来实现。Ceph块存储机制提供了RADOS（Reliable Autonomic Distributed Object Store）服务。RADOS服务存储机制中不可或缺的；RADOS服务通过使用节点中安装的软件管理工具能够扩展千级的硬件设备（通常被应用为"Nodes"）。
### Ceph Block Storage
Ceph的块存储模式使用户可以像挂载一个小型块设备一样挂载Ceph。在块数据存储级别上，RADOS服务也保证块数据的可扩展性。Librados就是包含在这一级别上的一个python类库，你可以使用librados类库和存储服务器或节点进行通信。Librados是一个开源的应用，你可以调整和增强它。Librados通过“RADOS Block Device“即RBD与后台进行交互。RBD不仅继承了Librados的功能，还能够为集群建立快照和恢复数据。
### Ceph File Storage
CephFS 是一个为Ceph集群设计的，且遵循POSIX标准的分布式文件系统。CephFS提供把数据目录和文件映射到存储在RADOS中对象的存储的服务。通过这种方式，CephFS和RADOS可以相互协作。在这里，RADOS动态均等地把数据分布到不同的节点上。这种文件系统支持无限的数据存储和更强的数据安全性。在文件存储集群系统中，Ceph因提供容量大和高可扩展性而闻名。
## 优缺点
## pros
#### 1. CRUSH算法
CRUSH算法是Ceph最初的两大创新之一（另一个是基于动态子树分区的元数据集群），也是整个RADOS的基石，是Ceph最引以为豪的地方。
CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。同时， CRUSH算法支持副本和EC两种数据冗余方式，还提供了四种不同类型的Bucket（Uniform, List, Tree, Straw），充分考虑了实际生产过程中硬件的迭代式部署方式，虽然实际生产中大多数情况下的都是只用了一种Straw。
另外根据Sage的论文，CRUSH算法具有相当好的可扩展性，在数千OSD的情况下仍然能保证良好的负载平衡。但这更多是理论层面的，目前还没有人给出在数PB规模的生产环境中的测试结果。
总的来看，CRUSH算法仍然是目前经过实践检验的最好的数据分布算法之一。
#### 2. 统一存储架构
Ceph最初设计的RADOS是为其实现一个高性能的文件系统服务的，并没有考虑对于块设备、对象存储的支持，也就没有什么RBD、RADOS GateWay，跟别提OpenStack和qemu之类的了。但谁想无心插柳柳成荫，由于 RADOS 出色的设计和独立简洁的访问接口，再加上Sage敏锐的眼光，Ceph社区果断推出了用于支持云计算的分布式块存储RBD和分布式对象存储RADOS GateWay，并将开发中心全面转向云计算领域。
不得不说，RADOS的设计还是非常的优秀。从架构上来看，RBD和RADOSGateWay实际上都只是RADOS的客户端而已，但得益于RADOS的优秀设计，RBD和RADOSGateWay的设计和实现都很简单，不需要考虑横向扩展、冗余、容灾、负载平衡的等复杂的分布式系统问题，同时能够提供足够多的特性和足够优秀的性能，因此迅速得到了社区的认可。另一方面，Ceph为OpenStack提供了良好的支持，成为了目前最火的OpenStack底层存储系统。乘着云计算和OpenStack的东风，Ceph作为一个统一存储系统，似乎大有舍我取谁之势。
#### 3. 丰富的特性
Ceph的特性不可谓不多，从分布式系统最基本的横向扩展、动态伸缩、冗余容灾、负载平衡等，到生产环境环境中非常实用的滚动升级、多存储池、延迟删除等，再到高大上的CephFS集群、快照、纠删码、跨存储池缓存等，不可谓不强大。
但是就像大多数开源系统一样，Ceph的基本特性，或者说真正在生产环境中用的上的特性还是非常靠谱的，但其他“高级”特性就只能打一个问号了。特别是在CephFS模块，由于Ceph社区目前的开发重点主要还是与云计算相关的部分，即RBD和RADOSGateWay，导致CephFS的开发停滞了很久，相关的特性，例如元数据集群、快照等，目前都不满足生产环境的要求。
## cons
#### 1. 性能
Ceph的性能总的来说还是不错的，基本上能发挥出物理硬件的性能，但是存在以下几个隐患：

* 数据双倍写入。Ceph本地存储接口（FileStore）为了支持事务，引入了日志（Journal）机制。所有的写入操作都需要先写入日志（XFS模式下），然后再写入本地文件系统。简单来说就是一份数据需要写两遍，日志+本地文件系统。这就造成了在大规模连续IO的情况下，实际上磁盘输出的吞吐量只有其物理性能的一半。

* IO路径过长。这个问题在Ceph的客户端和服务器端都存在。以osd为例，一个IO需要经过message、OSD、FileJournal、FileStore多个模块才能完成，每个模块之间都涉及到队列和线程切换，部分模块在对IO进行处理时还要进行内存拷贝，导致整体性能不高。

* 对高性能硬件的支持有待改进。Ceph最开始是为HDD设计的，没有充分考虑全SSD，甚至更先进的PCIe SSD和NVRAM的情况NVRAM。导致这些硬件的物理性能在Ceph中无法充分发挥出来，特别是延迟和IOPS，受比较大的影响。
#### 2. CephFS
CephFS现在在整个Ceph系统中处于一个较为尴尬的情况，因为POSIX这种借口似乎在云计算中没有用武之地，导致了社区对这个模块的关注不足，也就没有什么进展。
CephFS作为最初Ceph的设计目标，Sage投入了巨大的精力，几乎实现了所有需要的特性，并且进行了大量工程层面的优化。
正所谓成也萧何败萧何，Ceph想把CephFS模块做到足够强大，甚至是最强大，但强大的同时也意味着不菲的代价。元数据动态子树分区、目录分片、快照、权限控制、IOPS优化、故障恢复、分布式缓存、强弱一致性控制，这些高大上的名词背后都意味着复杂的工程性任务，更不要说将这些叠加在一起。很多时候，叠加不是想加，而是相乘的关系。最终的结果就是整个MDS的工程难度已经超过了可以掌控的程度，无法做出足够成熟、稳定的系统。
目前CephFS宣称其单MDS的模式是稳定的，MDS的集群的模式是不稳定的。而快照功能默认关闭，今后也够呛会有开启的可能了。
#### 3. 业务连续性
Ceph中的RADOS采用强一致性设计，即Write-All-Read-One，这种模式的好处在于读取效率较高，而且工程难度较低，比较适合与读多写少的系统。
当集群非常非常大时，Write-All-Read-One对于硬件可靠性的要求几乎是无法满足的。想象一下一个10PB的系统，按照最大4TB每块盘的计算，就有2500块磁盘。按照我们以往的运维经验，每周存在一块磁盘故障是完全正常的。这种场景下，如果数据分布足够分散，实际上一块磁盘可能涉及到很多数据块，也就是说一块磁盘故障会影响很多IO，而这种情况每周发生一次。这对业务连续性的影响是已经是不可忽略的。
生产环境中的场景比这个更加复杂，因为磁盘或者硬件的故障可能不仅表现为不可写，还有可能是慢或者不稳定。这些情况对于业务连续性的影响也更加严重。

## 主要指标
* Processor

ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。

ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。

ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。
* 内存

ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。
* 网络规划

万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。

* SSD选择

硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，Intel® SSD DC S3500 Series基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。

如何确定SSD是否适合作为SSD Journal，可以参考[SÉBASTIEN HAN的Ceph: How to Test if Your SSD Is Suitable as a Journal Device?](http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/)，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。

## My comments
Red Hat下的Ceph文件系统拥有性价比高、操作简单、集群数据高可靠性的特点,但部分功能还不够成熟。CephFS 是 Ceph 世界里下一个被期待的重大事件，它使得 Ceph 在统一存储的路上可以跨出一大步来完成庞大生态系统的闭环化。总的来说，Ceph瑕不掩瑜，仍然是一个优秀，甚至出色的开源存储系统。